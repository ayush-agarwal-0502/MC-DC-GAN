{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"DCGAN implemented on CIFAR10 Dataset \n\nBy Ayush Agarwal \n\n20095021\n\nElectronics Engineering \n\nIIT BHU Varanasi \n\nSubmission to the question enlisted in COPS Intelligence group questions ","metadata":{}},{"cell_type":"code","source":"# DCGAN on CIFAR10 \n# Ayush Agarwal , Electronics , IIT BHU Varanasi \n###############################################################\n# importing the libraries \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\nimport tensorflow as tf\nfrom IPython import display\n\n######################################################################################\n\n# Loading the dataset and making the initial changes\n\ncifar10 = tf.keras.datasets.cifar10\n(train_images, train_labels), (X_test, y_test) = cifar10.load_data()\ntrain_images = train_images.reshape(train_images.shape[0], 32, 32, 3).astype('float32')\ntrain_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\nBUFFER_SIZE = 50000\nBATCH_SIZE = 256\n# Batch and shuffle the data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n########################################################################################\n\n# The Generator model making function \n\ndef make_generator_model():\n    #merged_layer = Concatenate()([z, labels])\n    model = tf.keras.Sequential()\n    #model.add(tf.layers.concatenate([z, labels]))\n    #model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n    model.add(layers.Dense(8*8*3*256, use_bias=False, input_shape=(100,)))\n    #model.add(layers.Dense(8*8*3*256, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((8, 8, 256*3)))\n    assert model.output_shape == (None, 8, 8, 256*3)  # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 8, 8, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 16, 16, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 32, 32, 3)\n\n    return model\n\n#####################################################################################\n# Creating the generator \n\n#Use the (as yet untrained) generator to create an image.\ngenerator = make_generator_model()\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n#plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n###############################################################################################\n\n# The Discriminator Model making function \n\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[32, 32, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n####################################################################################\n# Creating the discriminator \n\n#Use the (as yet untrained) discriminator to classify the generated images as real or fake. \n#The model will be trained to output positive values for real images, and negative values for fake images.\ndiscriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\n\n###################################################################################\n\n# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n# the loss formulas (used cross entropy here )\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n# Choosing the optimisers \ngenerator_optimizer = tf.keras.optimizers.Adam(1e-2)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-2)\n\n################################################ CHANGE EPOCHS FROM HERE \n# Setting some other parameters \nEPOCHS = 5\nnoise_dim = 100\nnum_examples_to_generate = 16\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\n\n@tf.function\n\n\n# The processes that a single step of training involves \ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n# The train function to train the GAN \ndef train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n    for image_batch in dataset:\n      train_step(image_batch)\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n# This function does what it is named \ndef generate_and_save_images(model, epoch, test_input):\n  predictions = model(test_input, training=False)\n  fig = plt.figure(figsize=(4, 4))\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)#, cmap='gray'\n      plt.axis('off')\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()\n    \n# A function to display a single image using the epoch number\ndef display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n    \n# Training the GAN \ntrain(train_dataset, EPOCHS)\n\ndisplay_image(EPOCHS)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-05T13:31:39.32625Z","iopub.execute_input":"2022-01-05T13:31:39.326528Z","iopub.status.idle":"2022-01-05T13:32:08.576931Z","shell.execute_reply.started":"2022-01-05T13:31:39.326496Z","shell.execute_reply":"2022-01-05T13:32:08.575936Z"},"trusted":true},"execution_count":null,"outputs":[]}]}